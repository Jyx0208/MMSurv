# 05 - æµ‹è¯•å’ŒéªŒè¯

## æ¦‚è¿°

æœ¬é˜¶æ®µå°†å»ºç«‹MMSurvé¡¹ç›®çš„å®Œæ•´æµ‹è¯•å’ŒéªŒè¯ä½“ç³»ï¼Œç¡®ä¿é¡¹ç›®çš„å¯é æ€§ã€ç¨³å®šæ€§å’Œæ­£ç¡®æ€§ã€‚åŒ…æ‹¬å•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•ã€å›å½’æµ‹è¯•å’ŒæŒç»­é›†æˆç­‰ã€‚

## æµ‹è¯•ä½“ç³»æ¶æ„

### æµ‹è¯•å±‚æ¬¡
1. **å•å…ƒæµ‹è¯•** - æµ‹è¯•å•ä¸ªå‡½æ•°å’Œç±»çš„åŠŸèƒ½
2. **é›†æˆæµ‹è¯•** - æµ‹è¯•æ¨¡å—é—´çš„äº¤äº’
3. **ç³»ç»Ÿæµ‹è¯•** - æµ‹è¯•å®Œæ•´çš„ç«¯åˆ°ç«¯æµç¨‹
4. **æ€§èƒ½æµ‹è¯•** - æµ‹è¯•ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºä½¿ç”¨
5. **å›å½’æµ‹è¯•** - ç¡®ä¿æ–°æ›´æ”¹ä¸ç ´åç°æœ‰åŠŸèƒ½
6. **éªŒæ”¶æµ‹è¯•** - éªŒè¯ç³»ç»Ÿæ»¡è¶³éœ€æ±‚

### æµ‹è¯•å·¥å…·
- **pytest** - ä¸»è¦æµ‹è¯•æ¡†æ¶
- **coverage** - ä»£ç è¦†ç›–ç‡åˆ†æ
- **hypothesis** - å±æ€§åŸºç¡€æµ‹è¯•
- **mock** - æ¨¡æ‹Ÿå¯¹è±¡å’Œä¾èµ–
- **benchmark** - æ€§èƒ½åŸºå‡†æµ‹è¯•

## å®ç°ä»»åŠ¡

### ä»»åŠ¡1ï¼šå»ºç«‹æµ‹è¯•æ¡†æ¶

#### 1.1 åˆ›å»ºæµ‹è¯•ç›®å½•ç»“æ„
**ç›®å½•ç»“æ„**ï¼š
```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py                 # pytesté…ç½®å’Œfixtures
â”œâ”€â”€ unit/                       # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_models/           # æ¨¡å‹æµ‹è¯•
â”‚   â”œâ”€â”€ test_datasets/         # æ•°æ®é›†æµ‹è¯•
â”‚   â”œâ”€â”€ test_utils/            # å·¥å…·å‡½æ•°æµ‹è¯•
â”‚   â””â”€â”€ test_visualization/    # å¯è§†åŒ–æµ‹è¯•
â”œâ”€â”€ integration/               # é›†æˆæµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_training_pipeline.py
â”‚   â”œâ”€â”€ test_evaluation_pipeline.py
â”‚   â””â”€â”€ test_data_pipeline.py
â”œâ”€â”€ system/                    # ç³»ç»Ÿæµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_end_to_end.py
â”‚   â””â”€â”€ test_cross_validation.py
â”œâ”€â”€ performance/               # æ€§èƒ½æµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_model_speed.py
â”‚   â”œâ”€â”€ test_memory_usage.py
â”‚   â””â”€â”€ test_scalability.py
â””â”€â”€ fixtures/                  # æµ‹è¯•æ•°æ®å’Œfixtures
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ sample_data.py
    â””â”€â”€ mock_models.py
```

#### 1.2 åˆ›å»ºpytesté…ç½®
**æ–‡ä»¶ä½ç½®**ï¼š`tests/conftest.py`

**å®ç°è¦æ±‚**ï¼š
- é…ç½®pytestè®¾ç½®
- å®šä¹‰é€šç”¨fixtures
- è®¾ç½®æµ‹è¯•ç¯å¢ƒ
- é…ç½®æ—¥å¿—å’ŒæŠ¥å‘Š

#### 1.3 åˆ›å»ºæµ‹è¯•å·¥å…·
**æ–‡ä»¶ä½ç½®**ï¼š`tests/test_utils.py`

**å®ç°è¦æ±‚**ï¼š
- å®ç°æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨
- å®ç°æ–­è¨€è¾…åŠ©å‡½æ•°
- å®ç°æ€§èƒ½æµ‹è¯•å·¥å…·
- å®ç°æ¨¡æ‹Ÿå¯¹è±¡å·¥å‚

#### 1.4 æµ‹è¯•æ¡†æ¶éªŒè¯
**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºåŸºç¡€æµ‹è¯•æ¡†æ¶éªŒè¯
cat > test_framework_setup.py << 'EOF'
import pytest
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.abspath('.'))

def test_pytest_working():
    """éªŒè¯pyteståŸºç¡€åŠŸèƒ½"""
    assert True
    
def test_imports():
    """éªŒè¯æ ¸å¿ƒæ¨¡å—å¯ä»¥å¯¼å…¥"""
    try:
        import mmsurv
        import mmsurv.models
        import mmsurv.datasets
        import mmsurv.utils
        print("æ‰€æœ‰æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        pytest.fail(f"æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        
def test_torch_available():
    """éªŒè¯PyTorchå¯ç”¨æ€§"""
    import torch
    assert torch.cuda.is_available() or True  # CPUä¹Ÿå¯ä»¥
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
EOF

python test_framework_setup.py
```

### ä»»åŠ¡2ï¼šå®ç°å•å…ƒæµ‹è¯•

#### 2.1 æ¨¡å‹å•å…ƒæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/unit/test_models/`

##### 2.1.1 PORPOISEæ¨¡å‹æµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/unit/test_models/test_porpoise.py`

**æµ‹è¯•å†…å®¹**ï¼š
- æ¨¡å‹åˆå§‹åŒ–æµ‹è¯•
- å‰å‘ä¼ æ’­æµ‹è¯•
- å‚æ•°æ•°é‡éªŒè¯
- è¾“å‡ºç»´åº¦æ£€æŸ¥
- æ¢¯åº¦æµæµ‹è¯•

**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºPORPOISEæ¨¡å‹å•å…ƒæµ‹è¯•
cat > test_porpoise_unit.py << 'EOF'
import pytest
import torch
from mmsurv.models.model_porpoise import PorpoiseMMF, LRBilinearFusion

class TestPorpoiseMMF:
    """PORPOISEæ¨¡å‹å•å…ƒæµ‹è¯•"""
    
    @pytest.fixture
    def model_config(self):
        return {
            'omic_input_dim': 50,
            'path_input_dim': 768,
            'n_classes': 4,
            'dropout': 0.25
        }
    
    @pytest.fixture
    def sample_data(self):
        return {
            'path_features': torch.randn(2, 100, 768),
            'omic_features': torch.randn(2, 50)
        }
    
    def test_model_initialization(self, model_config):
        """æµ‹è¯•æ¨¡å‹åˆå§‹åŒ–"""
        model = PorpoiseMMF(**model_config)
        assert model is not None
        assert hasattr(model, 'path_fc')
        assert hasattr(model, 'omic_fc')
        
    def test_forward_pass(self, model_config, sample_data):
        """æµ‹è¯•å‰å‘ä¼ æ’­"""
        model = PorpoiseMMF(**model_config)
        model.eval()
        
        with torch.no_grad():
            output = model(
                h_path=sample_data['path_features'],
                h_omic=sample_data['omic_features']
            )
        
        assert output.shape == (2, 4)  # batch_size=2, n_classes=4
        assert not torch.isnan(output).any()
        
    def test_parameter_count(self, model_config):
        """æµ‹è¯•å‚æ•°æ•°é‡"""
        model = PorpoiseMMF(**model_config)
        param_count = sum(p.numel() for p in model.parameters())
        
        # PORPOISEåº”è¯¥æ˜¯å‚æ•°æ•ˆç‡é«˜çš„æ¨¡å‹
        assert param_count < 10_000_000  # å°‘äº1000ä¸‡å‚æ•°
        print(f"PORPOISEå‚æ•°æ•°é‡: {param_count:,}")
        
    def test_gradient_flow(self, model_config, sample_data):
        """æµ‹è¯•æ¢¯åº¦æµ"""
        model = PorpoiseMMF(**model_config)
        model.train()
        
        output = model(
            h_path=sample_data['path_features'],
            h_omic=sample_data['omic_features']
        )
        
        # è®¡ç®—æŸå¤±å¹¶åå‘ä¼ æ’­
        target = torch.randn_like(output)
        loss = torch.nn.MSELoss()(output, target)
        loss.backward()
        
        # æ£€æŸ¥æ¢¯åº¦
        has_grad = False
        for param in model.parameters():
            if param.grad is not None and param.grad.abs().sum() > 0:
                has_grad = True
                break
        
        assert has_grad, "æ¨¡å‹å‚æ•°æ²¡æœ‰æ¢¯åº¦"
        
class TestLRBilinearFusion:
    """ä½ç§©åŒçº¿æ€§èåˆæµ‹è¯•"""
    
    def test_fusion_initialization(self):
        """æµ‹è¯•èåˆå±‚åˆå§‹åŒ–"""
        fusion = LRBilinearFusion(dim1=768, dim2=50, scale_dim1=8, scale_dim2=8)
        assert fusion is not None
        
    def test_fusion_forward(self):
        """æµ‹è¯•èåˆå±‚å‰å‘ä¼ æ’­"""
        fusion = LRBilinearFusion(dim1=768, dim2=50, scale_dim1=8, scale_dim2=8)
        
        vec1 = torch.randn(2, 768)
        vec2 = torch.randn(2, 50)
        
        output = fusion(vec1, vec2)
        assert output.shape == (2, 8)  # scale_dim1
        
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
EOF

python test_porpoise_unit.py
```

##### 2.1.2 å…¶ä»–æ¨¡å‹æµ‹è¯•
**æµ‹è¯•æ–‡ä»¶**ï¼š
- `test_mcat.py` - MCATæ¨¡å‹æµ‹è¯•
- `test_motcat.py` - MOTCatæ¨¡å‹æµ‹è¯•
- `test_mil_models.py` - MILæ¨¡å‹ç³»åˆ—æµ‹è¯•
- `test_genomic.py` - åŸºå› ç»„æ¨¡å‹æµ‹è¯•

#### 2.2 æ•°æ®é›†å•å…ƒæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/unit/test_datasets/`

##### 2.2.1 ç”Ÿå­˜æ•°æ®é›†æµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/unit/test_datasets/test_survival_dataset.py`

**æµ‹è¯•å†…å®¹**ï¼š
- æ•°æ®é›†åˆå§‹åŒ–æµ‹è¯•
- æ•°æ®åŠ è½½æµ‹è¯•
- æ ‡ç­¾å¤„ç†æµ‹è¯•
- æ•°æ®å¢å¼ºæµ‹è¯•
- æ‰¹å¤„ç†æµ‹è¯•

**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºæ•°æ®é›†å•å…ƒæµ‹è¯•
cat > test_dataset_unit.py << 'EOF'
import pytest
import torch
import numpy as np
import pandas as pd
from torch.utils.data import DataLoader
from mmsurv.datasets.dataset_survival import Generic_WSI_Survival_Dataset

class TestSurvivalDataset:
    """ç”Ÿå­˜æ•°æ®é›†å•å…ƒæµ‹è¯•"""
    
    @pytest.fixture
    def mock_csv_data(self, tmp_path):
        """åˆ›å»ºæ¨¡æ‹ŸCSVæ•°æ®"""
        data = {
            'case_id': [f'case_{i}' for i in range(10)],
            'slide_id': [f'slide_{i}' for i in range(10)],
            'survival_months': np.random.uniform(1, 100, 10),
            'censorship': np.random.randint(0, 2, 10),
            'oncotree_code': ['LUAD'] * 10
        }
        
        df = pd.DataFrame(data)
        csv_path = tmp_path / "test_data.csv"
        df.to_csv(csv_path, index=False)
        return csv_path
    
    @pytest.fixture
    def mock_features_dir(self, tmp_path):
        """åˆ›å»ºæ¨¡æ‹Ÿç‰¹å¾ç›®å½•"""
        features_dir = tmp_path / "features"
        features_dir.mkdir()
        
        # åˆ›å»ºæ¨¡æ‹Ÿç‰¹å¾æ–‡ä»¶
        for i in range(10):
            feature_file = features_dir / f"slide_{i}.pt"
            features = torch.randn(50, 768)  # 50ä¸ªpatchï¼Œ768ç»´ç‰¹å¾
            torch.save(features, feature_file)
            
        return features_dir
    
    def test_dataset_initialization(self, mock_csv_data, mock_features_dir):
        """æµ‹è¯•æ•°æ®é›†åˆå§‹åŒ–"""
        dataset = Generic_WSI_Survival_Dataset(
            csv_file=str(mock_csv_data),
            data_dir=str(mock_features_dir),
            shuffle=False,
            seed=42,
            print_info=True,
            n_bins=4,
            ignore=[]
        )
        
        assert len(dataset) == 10
        assert dataset.num_classes == 4
        
    def test_dataset_getitem(self, mock_csv_data, mock_features_dir):
        """æµ‹è¯•æ•°æ®è·å–"""
        dataset = Generic_WSI_Survival_Dataset(
            csv_file=str(mock_csv_data),
            data_dir=str(mock_features_dir),
            shuffle=False,
            seed=42,
            n_bins=4
        )
        
        # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        sample = dataset[0]
        
        assert 'features' in sample
        assert 'label' in sample
        assert 'event_time' in sample
        assert 'c' in sample  # censorship
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        features = sample['features']
        assert features.shape[1] == 768  # ç‰¹å¾ç»´åº¦
        
    def test_dataloader_integration(self, mock_csv_data, mock_features_dir):
        """æµ‹è¯•æ•°æ®åŠ è½½å™¨é›†æˆ"""
        dataset = Generic_WSI_Survival_Dataset(
            csv_file=str(mock_csv_data),
            data_dir=str(mock_features_dir),
            shuffle=False,
            seed=42,
            n_bins=4
        )
        
        dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
        
        # æµ‹è¯•æ‰¹æ¬¡åŠ è½½
        batch = next(iter(dataloader))
        
        assert len(batch) == 4  # features, label, event_time, censorship
        assert batch[0].shape[0] == 2  # batch_size
        
    def test_survival_binning(self, mock_csv_data, mock_features_dir):
        """æµ‹è¯•ç”Ÿå­˜æ—¶é—´åˆ†ç®±"""
        dataset = Generic_WSI_Survival_Dataset(
            csv_file=str(mock_csv_data),
            data_dir=str(mock_features_dir),
            shuffle=False,
            seed=42,
            n_bins=4
        )
        
        # æ£€æŸ¥æ ‡ç­¾èŒƒå›´
        labels = [dataset[i]['label'] for i in range(len(dataset))]
        unique_labels = set(labels)
        
        assert all(0 <= label < 4 for label in labels)
        print(f"å”¯ä¸€æ ‡ç­¾: {sorted(unique_labels)}")
        
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
EOF

python test_dataset_unit.py
```

#### 2.3 å·¥å…·å‡½æ•°å•å…ƒæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/unit/test_utils/`

##### 2.3.1 è¯„ä¼°å·¥å…·æµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/unit/test_utils/test_eval_utils.py`

**æµ‹è¯•å†…å®¹**ï¼š
- C-Indexè®¡ç®—æµ‹è¯•
- AUCè®¡ç®—æµ‹è¯•
- ç»Ÿè®¡æ£€éªŒæµ‹è¯•
- è¾¹ç•Œæ¡ä»¶æµ‹è¯•

**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºè¯„ä¼°å·¥å…·å•å…ƒæµ‹è¯•
cat > test_eval_utils_unit.py << 'EOF'
import pytest
import torch
import numpy as np
from mmsurv.utils.eval_utils import calculate_c_index
from mmsurv.utils.statistical_tests import paired_t_test

class TestCIndexCalculation:
    """C-Indexè®¡ç®—æµ‹è¯•"""
    
    def test_perfect_concordance(self):
        """æµ‹è¯•å®Œç¾ä¸€è‡´æ€§æƒ…å†µ"""
        # é£é™©è¯„åˆ†ä¸äº‹ä»¶æ—¶é—´å®Œå…¨è´Ÿç›¸å…³
        risk_scores = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])
        event_times = torch.tensor([5.0, 4.0, 3.0, 2.0, 1.0])
        censorship = torch.tensor([1, 1, 1, 1, 1])  # æ— åˆ å¤±
        
        c_index = calculate_c_index(risk_scores, event_times, censorship)
        assert abs(c_index - 1.0) < 1e-6, f"æœŸæœ›C-Index=1.0ï¼Œå®é™…={c_index}"
        
    def test_random_concordance(self):
        """æµ‹è¯•éšæœºä¸€è‡´æ€§æƒ…å†µ"""
        np.random.seed(42)
        torch.manual_seed(42)
        
        n_samples = 1000
        risk_scores = torch.randn(n_samples)
        event_times = torch.rand(n_samples) * 100
        censorship = torch.ones(n_samples)  # æ— åˆ å¤±
        
        c_index = calculate_c_index(risk_scores, event_times, censorship)
        
        # éšæœºæƒ…å†µä¸‹C-Indexåº”è¯¥æ¥è¿‘0.5
        assert 0.4 < c_index < 0.6, f"éšæœºC-Indexåº”æ¥è¿‘0.5ï¼Œå®é™…={c_index}"
        
    def test_with_censoring(self):
        """æµ‹è¯•åŒ…å«åˆ å¤±çš„æƒ…å†µ"""
        risk_scores = torch.tensor([1.0, 2.0, 3.0, 4.0])
        event_times = torch.tensor([10.0, 8.0, 6.0, 4.0])
        censorship = torch.tensor([1, 0, 1, 1])  # ç¬¬äºŒä¸ªæ ·æœ¬è¢«åˆ å¤±
        
        c_index = calculate_c_index(risk_scores, event_times, censorship)
        
        # åº”è¯¥èƒ½æ­£å¸¸è®¡ç®—ï¼Œä¸æŠ¥é”™
        assert 0 <= c_index <= 1, f"C-Indexåº”åœ¨[0,1]èŒƒå›´å†…ï¼Œå®é™…={c_index}"
        
    def test_edge_cases(self):
        """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
        # æ‰€æœ‰æ ·æœ¬éƒ½è¢«åˆ å¤±
        risk_scores = torch.tensor([1.0, 2.0, 3.0])
        event_times = torch.tensor([10.0, 8.0, 6.0])
        censorship = torch.tensor([0, 0, 0])  # å…¨éƒ¨åˆ å¤±
        
        c_index = calculate_c_index(risk_scores, event_times, censorship)
        
        # å…¨éƒ¨åˆ å¤±æ—¶ï¼ŒC-Indexåº”è¯¥æ˜¯NaNæˆ–ç‰¹æ®Šå€¼
        assert torch.isnan(torch.tensor(c_index)) or c_index == 0.5
        
class TestStatisticalTests:
    """ç»Ÿè®¡æ£€éªŒæµ‹è¯•"""
    
    def test_paired_t_test_identical(self):
        """æµ‹è¯•ç›¸åŒæ•°æ®çš„é…å¯¹tæ£€éªŒ"""
        data1 = np.array([1, 2, 3, 4, 5])
        data2 = np.array([1, 2, 3, 4, 5])
        
        t_stat, p_value = paired_t_test(data1, data2)
        
        assert abs(t_stat) < 1e-10, f"ç›¸åŒæ•°æ®tç»Ÿè®¡é‡åº”ä¸º0ï¼Œå®é™…={t_stat}"
        assert p_value > 0.9, f"ç›¸åŒæ•°æ®på€¼åº”æ¥è¿‘1ï¼Œå®é™…={p_value}"
        
    def test_paired_t_test_different(self):
        """æµ‹è¯•ä¸åŒæ•°æ®çš„é…å¯¹tæ£€éªŒ"""
        np.random.seed(42)
        data1 = np.random.normal(0, 1, 100)
        data2 = np.random.normal(1, 1, 100)  # å‡å€¼å·®ä¸º1
        
        t_stat, p_value = paired_t_test(data1, data2)
        
        assert abs(t_stat) > 2, f"æ˜¾è‘—å·®å¼‚çš„tç»Ÿè®¡é‡åº”è¾ƒå¤§ï¼Œå®é™…={abs(t_stat)}"
        assert p_value < 0.05, f"æ˜¾è‘—å·®å¼‚çš„på€¼åº”å°äº0.05ï¼Œå®é™…={p_value}"
        
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
EOF

python test_eval_utils_unit.py
```

### ä»»åŠ¡3ï¼šå®ç°é›†æˆæµ‹è¯•

#### 3.1 è®­ç»ƒæµç¨‹é›†æˆæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/integration/test_training_pipeline.py`

**æµ‹è¯•å†…å®¹**ï¼š
- å®Œæ•´è®­ç»ƒæµç¨‹æµ‹è¯•
- å¤šæ¨¡å‹è®­ç»ƒæµ‹è¯•
- é…ç½®æ–‡ä»¶é›†æˆæµ‹è¯•
- æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•

**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºè®­ç»ƒæµç¨‹é›†æˆæµ‹è¯•
cat > test_training_integration.py << 'EOF'
import pytest
import torch
import tempfile
import os
from mmsurv.models.model_porpoise import PorpoiseMMF
from mmsurv.utils.core_utils import train, EarlyStopping
from mmsurv.utils.loss_func import CrossEntropySurvLoss
from torch.utils.data import DataLoader, TensorDataset

class TestTrainingPipeline:
    """è®­ç»ƒæµç¨‹é›†æˆæµ‹è¯•"""
    
    @pytest.fixture
    def mock_data_loaders(self):
        """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®åŠ è½½å™¨"""
        n_samples = 20
        path_features = torch.randn(n_samples, 50, 768)
        omic_features = torch.randn(n_samples, 50)
        labels = torch.randint(0, 4, (n_samples,))
        event_times = torch.rand(n_samples)
        censorship = torch.randint(0, 2, (n_samples,))
        
        dataset = TensorDataset(
            path_features, omic_features, labels, event_times, censorship
        )
        
        train_loader = DataLoader(dataset, batch_size=4, shuffle=True)
        val_loader = DataLoader(dataset, batch_size=4, shuffle=False)
        
        return train_loader, val_loader
    
    def test_basic_training_loop(self, mock_data_loaders):
        """æµ‹è¯•åŸºç¡€è®­ç»ƒå¾ªç¯"""
        train_loader, val_loader = mock_data_loaders
        
        # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
        model = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        loss_fn = torch.nn.CrossEntropyLoss()
        
        # ç®€åŒ–è®­ç»ƒå¾ªç¯
        model.train()
        initial_loss = None
        final_loss = None
        
        for epoch in range(3):  # åªè®­ç»ƒ3è½®
            epoch_loss = 0
            num_batches = 0
            
            for batch_idx, (path_feat, omic_feat, labels, _, _) in enumerate(train_loader):
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                output = model(h_path=path_feat, h_omic=omic_feat)
                loss = loss_fn(output, labels)
                
                # åå‘ä¼ æ’­
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                num_batches += 1
                
                if batch_idx >= 2:  # é™åˆ¶æ‰¹æ¬¡æ•°é‡
                    break
            
            avg_loss = epoch_loss / num_batches
            
            if epoch == 0:
                initial_loss = avg_loss
            if epoch == 2:
                final_loss = avg_loss
                
            print(f"Epoch {epoch+1}: å¹³å‡æŸå¤± = {avg_loss:.4f}")
        
        # éªŒè¯è®­ç»ƒæœ‰æ•ˆæ€§
        assert initial_loss is not None and final_loss is not None
        print(f"åˆå§‹æŸå¤±: {initial_loss:.4f}, æœ€ç»ˆæŸå¤±: {final_loss:.4f}")
        
    def test_early_stopping_integration(self, mock_data_loaders):
        """æµ‹è¯•æ—©åœæœºåˆ¶é›†æˆ"""
        train_loader, val_loader = mock_data_loaders
        
        model = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        early_stopping = EarlyStopping(patience=2, min_delta=0.001)
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
        val_losses = [1.0, 0.9, 0.91, 0.92, 0.93]  # éªŒè¯æŸå¤±å¼€å§‹ä¸Šå‡
        
        for epoch, val_loss in enumerate(val_losses):
            should_stop = early_stopping(val_loss)
            print(f"Epoch {epoch+1}: val_loss={val_loss:.3f}, should_stop={should_stop}")
            
            if should_stop:
                print(f"æ—©åœè§¦å‘äºç¬¬{epoch+1}è½®")
                assert epoch >= 2  # åº”è¯¥åœ¨ç¬¬3è½®æˆ–ä¹‹åè§¦å‘
                break
        
    def test_checkpoint_save_load(self, mock_data_loaders):
        """æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½"""
        train_loader, val_loader = mock_data_loaders
        
        with tempfile.TemporaryDirectory() as temp_dir:
            # åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
            model = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            
            # ä¿å­˜åˆå§‹çŠ¶æ€
            initial_state = model.state_dict().copy()
            
            # ç®€å•è®­ç»ƒå‡ æ­¥
            model.train()
            for batch_idx, (path_feat, omic_feat, labels, _, _) in enumerate(train_loader):
                optimizer.zero_grad()
                output = model(h_path=path_feat, h_omic=omic_feat)
                loss = torch.nn.CrossEntropyLoss()(output, labels)
                loss.backward()
                optimizer.step()
                
                if batch_idx >= 1:  # åªè®­ç»ƒ2ä¸ªæ‰¹æ¬¡
                    break
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_path = os.path.join(temp_dir, 'checkpoint.pth')
            torch.save({
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'epoch': 1,
                'loss': loss.item()
            }, checkpoint_path)
            
            # åˆ›å»ºæ–°æ¨¡å‹å¹¶åŠ è½½æ£€æŸ¥ç‚¹
            new_model = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
            new_optimizer = torch.optim.Adam(new_model.parameters(), lr=0.001)
            
            checkpoint = torch.load(checkpoint_path)
            new_model.load_state_dict(checkpoint['model_state_dict'])
            new_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            
            # éªŒè¯åŠ è½½æˆåŠŸ
            assert checkpoint['epoch'] == 1
            assert isinstance(checkpoint['loss'], float)
            
            # éªŒè¯æ¨¡å‹å‚æ•°å·²æ›´æ”¹ï¼ˆä¸åˆå§‹çŠ¶æ€ä¸åŒï¼‰
            current_state = new_model.state_dict()
            params_changed = False
            for key in initial_state:
                if not torch.equal(initial_state[key], current_state[key]):
                    params_changed = True
                    break
            
            assert params_changed, "æ¨¡å‹å‚æ•°åº”è¯¥åœ¨è®­ç»ƒåå‘ç”Ÿå˜åŒ–"
            print("æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½æµ‹è¯•é€šè¿‡")
            
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
EOF

python test_training_integration.py
```

#### 3.2 è¯„ä¼°æµç¨‹é›†æˆæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/integration/test_evaluation_pipeline.py`

**æµ‹è¯•å†…å®¹**ï¼š
- å®Œæ•´è¯„ä¼°æµç¨‹æµ‹è¯•
- å¤šæ¨¡å‹æ¯”è¾ƒæµ‹è¯•
- å¯è§†åŒ–ç”Ÿæˆæµ‹è¯•
- æŠ¥å‘Šç”Ÿæˆæµ‹è¯•

#### 3.3 æ•°æ®æµç¨‹é›†æˆæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/integration/test_data_pipeline.py`

**æµ‹è¯•å†…å®¹**ï¼š
- æ•°æ®åŠ è½½æµç¨‹æµ‹è¯•
- æ•°æ®é¢„å¤„ç†æµç¨‹æµ‹è¯•
- æ‰¹å¤„ç†æµç¨‹æµ‹è¯•
- å†…å­˜ä½¿ç”¨æµ‹è¯•

### ä»»åŠ¡4ï¼šå®ç°ç³»ç»Ÿæµ‹è¯•

#### 4.1 ç«¯åˆ°ç«¯ç³»ç»Ÿæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/system/test_end_to_end.py`

**æµ‹è¯•å†…å®¹**ï¼š
- å®Œæ•´é¡¹ç›®æµç¨‹æµ‹è¯•
- å¤šæ¨¡å‹ç«¯åˆ°ç«¯æµ‹è¯•
- é…ç½®é©±åŠ¨æµ‹è¯•
- ç»“æœä¸€è‡´æ€§æµ‹è¯•

**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºç«¯åˆ°ç«¯ç³»ç»Ÿæµ‹è¯•
cat > test_e2e_system.py << 'EOF'
import pytest
import torch
import tempfile
import os
import yaml
from mmsurv.models.model_porpoise import PorpoiseMMF
from mmsurv.models.model_set_mil import MIL_Attention_FC_surv
from mmsurv.evaluation.model_comparator import ModelComparator
from torch.utils.data import DataLoader, TensorDataset

class TestEndToEndSystem:
    """ç«¯åˆ°ç«¯ç³»ç»Ÿæµ‹è¯•"""
    
    @pytest.fixture
    def system_config(self):
        """ç³»ç»Ÿé…ç½®"""
        return {
            'models': {
                'porpoise': {
                    'type': 'PorpoiseMMF',
                    'omic_input_dim': 50,
                    'path_input_dim': 768,
                    'n_classes': 4
                },
                'amil': {
                    'type': 'MIL_Attention_FC_surv',
                    'omic_input_dim': 50,
                    'path_input_dim': 768,
                    'n_classes': 4
                }
            },
            'training': {
                'batch_size': 4,
                'num_epochs': 2,
                'learning_rate': 0.001
            },
            'evaluation': {
                'metrics': ['c_index', 'auc'],
                'cross_validation': {
                    'n_splits': 3,
                    'shuffle': True,
                    'random_state': 42
                }
            }
        }
    
    @pytest.fixture
    def system_data(self):
        """ç³»ç»Ÿæµ‹è¯•æ•°æ®"""
        n_samples = 30
        return {
            'path_features': torch.randn(n_samples, 50, 768),
            'omic_features': torch.randn(n_samples, 50),
            'labels': torch.randint(0, 4, (n_samples,)),
            'event_times': torch.rand(n_samples) * 100,
            'censorship': torch.randint(0, 2, (n_samples,))
        }
    
    def test_multi_model_comparison(self, system_config, system_data):
        """æµ‹è¯•å¤šæ¨¡å‹æ¯”è¾ƒç³»ç»Ÿ"""
        # åˆ›å»ºæ¨¡å‹
        models = {
            'PORPOISE': PorpoiseMMF(
                omic_input_dim=50,
                path_input_dim=768,
                n_classes=4
            ),
            'AMIL': MIL_Attention_FC_surv(
                omic_input_dim=50,
                path_input_dim=768,
                n_classes=4
            )
        }
        
        # æ¨¡æ‹Ÿè®­ç»ƒï¼ˆç®€åŒ–ï¼‰
        for model_name, model in models.items():
            model.eval()  # è®¾ä¸ºè¯„ä¼°æ¨¡å¼
            print(f"æ¨¡å‹ {model_name} å‡†å¤‡å®Œæˆ")
        
        # æ¨¡æ‹Ÿè¯„ä¼°
        results = {}
        for model_name, model in models.items():
            with torch.no_grad():
                output = model(
                    h_path=system_data['path_features'],
                    h_omic=system_data['omic_features']
                )
                
                # æ¨¡æ‹Ÿè¯„ä¼°æŒ‡æ ‡
                results[model_name] = {
                    'c_index': torch.rand(1).item() * 0.3 + 0.5,  # 0.5-0.8
                    'auc': torch.rand(1).item() * 0.3 + 0.6,      # 0.6-0.9
                    'params': sum(p.numel() for p in model.parameters())
                }
        
        # éªŒè¯ç»“æœ
        assert len(results) == 2
        for model_name, metrics in results.items():
            assert 0.5 <= metrics['c_index'] <= 0.8
            assert 0.6 <= metrics['auc'] <= 0.9
            assert metrics['params'] > 0
            print(f"{model_name}: C-Index={metrics['c_index']:.3f}, AUC={metrics['auc']:.3f}")
        
        print("å¤šæ¨¡å‹æ¯”è¾ƒç³»ç»Ÿæµ‹è¯•é€šè¿‡")
    
    def test_config_driven_workflow(self, system_config, system_data):
        """æµ‹è¯•é…ç½®é©±åŠ¨çš„å·¥ä½œæµç¨‹"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # ä¿å­˜é…ç½®æ–‡ä»¶
            config_path = os.path.join(temp_dir, 'config.yaml')
            with open(config_path, 'w') as f:
                yaml.dump(system_config, f)
            
            # åŠ è½½é…ç½®
            with open(config_path, 'r') as f:
                loaded_config = yaml.safe_load(f)
            
            # éªŒè¯é…ç½®åŠ è½½
            assert loaded_config['models']['porpoise']['type'] == 'PorpoiseMMF'
            assert loaded_config['training']['batch_size'] == 4
            assert loaded_config['evaluation']['metrics'] == ['c_index', 'auc']
            
            # æ ¹æ®é…ç½®åˆ›å»ºæ¨¡å‹
            porpoise_config = loaded_config['models']['porpoise']
            model = PorpoiseMMF(
                omic_input_dim=porpoise_config['omic_input_dim'],
                path_input_dim=porpoise_config['path_input_dim'],
                n_classes=porpoise_config['n_classes']
            )
            
            # éªŒè¯æ¨¡å‹åˆ›å»º
            assert model is not None
            print("é…ç½®é©±åŠ¨å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡")
    
    def test_reproducibility(self, system_data):
        """æµ‹è¯•ç»“æœå¯é‡ç°æ€§"""
        # è®¾ç½®éšæœºç§å­
        torch.manual_seed(42)
        
        # ç¬¬ä¸€æ¬¡è¿è¡Œ
        model1 = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
        model1.eval()
        
        with torch.no_grad():
            output1 = model1(
                h_path=system_data['path_features'][:5],
                h_omic=system_data['omic_features'][:5]
            )
        
        # é‡ç½®éšæœºç§å­
        torch.manual_seed(42)
        
        # ç¬¬äºŒæ¬¡è¿è¡Œ
        model2 = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
        model2.eval()
        
        with torch.no_grad():
            output2 = model2(
                h_path=system_data['path_features'][:5],
                h_omic=system_data['omic_features'][:5]
            )
        
        # éªŒè¯ç»“æœä¸€è‡´æ€§
        assert torch.allclose(output1, output2, atol=1e-6), "ç»“æœåº”è¯¥å¯é‡ç°"
        print("å¯é‡ç°æ€§æµ‹è¯•é€šè¿‡")
        
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
EOF

python test_e2e_system.py
```

#### 4.2 äº¤å‰éªŒè¯ç³»ç»Ÿæµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/system/test_cross_validation.py`

**æµ‹è¯•å†…å®¹**ï¼š
- KæŠ˜äº¤å‰éªŒè¯å®Œæ•´æµç¨‹
- ç»Ÿè®¡æ˜¾è‘—æ€§æµ‹è¯•
- ç»“æœèšåˆå’ŒæŠ¥å‘Š
- æ€§èƒ½ç¨³å®šæ€§æµ‹è¯•

### ä»»åŠ¡5ï¼šå®ç°æ€§èƒ½æµ‹è¯•

#### 5.1 æ¨¡å‹é€Ÿåº¦æ€§èƒ½æµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/performance/test_model_speed.py`

**æµ‹è¯•å†…å®¹**ï¼š
- æ¨ç†é€Ÿåº¦åŸºå‡†æµ‹è¯•
- è®­ç»ƒé€Ÿåº¦åŸºå‡†æµ‹è¯•
- æ‰¹å¤„ç†æ€§èƒ½æµ‹è¯•
- GPUåŠ é€Ÿæµ‹è¯•

**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•
cat > test_performance_benchmark.py << 'EOF'
import pytest
import torch
import time
import psutil
import os
from mmsurv.models.model_porpoise import PorpoiseMMF
from mmsurv.models.model_coattn import MCAT_Surv
from torch.utils.data import DataLoader, TensorDataset

class TestModelPerformance:
    """æ¨¡å‹æ€§èƒ½æµ‹è¯•"""
    
    @pytest.fixture
    def performance_data(self):
        """æ€§èƒ½æµ‹è¯•æ•°æ®"""
        batch_sizes = [1, 4, 8, 16]
        data = {}
        
        for batch_size in batch_sizes:
            data[batch_size] = {
                'path_features': torch.randn(batch_size, 100, 768),
                'omic_features': torch.randn(batch_size, 50)
            }
        
        return data
    
    def test_inference_speed(self, performance_data):
        """æµ‹è¯•æ¨ç†é€Ÿåº¦"""
        model = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
        model.eval()
        
        results = {}
        
        for batch_size, data in performance_data.items():
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(5):
                    _ = model(h_path=data['path_features'], h_omic=data['omic_features'])
            
            # æ€§èƒ½æµ‹è¯•
            start_time = time.time()
            num_iterations = 50
            
            with torch.no_grad():
                for _ in range(num_iterations):
                    output = model(h_path=data['path_features'], h_omic=data['omic_features'])
            
            end_time = time.time()
            
            total_time = end_time - start_time
            avg_time_per_batch = total_time / num_iterations
            avg_time_per_sample = avg_time_per_batch / batch_size
            
            results[batch_size] = {
                'total_time': total_time,
                'avg_time_per_batch': avg_time_per_batch,
                'avg_time_per_sample': avg_time_per_sample,
                'throughput': batch_size * num_iterations / total_time
            }
            
            print(f"Batch Size {batch_size}:")
            print(f"  æ¯æ‰¹æ¬¡å¹³å‡æ—¶é—´: {avg_time_per_batch*1000:.2f}ms")
            print(f"  æ¯æ ·æœ¬å¹³å‡æ—¶é—´: {avg_time_per_sample*1000:.2f}ms")
            print(f"  ååé‡: {results[batch_size]['throughput']:.1f} samples/sec")
        
        # éªŒè¯æ€§èƒ½è¦æ±‚
        for batch_size, metrics in results.items():
            # æ¯æ ·æœ¬æ¨ç†æ—¶é—´åº”å°äº100ms
            assert metrics['avg_time_per_sample'] < 0.1, f"æ¨ç†é€Ÿåº¦è¿‡æ…¢: {metrics['avg_time_per_sample']:.3f}s"
        
        print("æ¨ç†é€Ÿåº¦æµ‹è¯•é€šè¿‡")
    
    def test_memory_usage(self, performance_data):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        def get_memory_usage():
            process = psutil.Process(os.getpid())
            return process.memory_info().rss / 1024 / 1024  # MB
        
        # åŸºå‡†å†…å­˜
        baseline_memory = get_memory_usage()
        
        # åˆ›å»ºæ¨¡å‹
        model = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
        model_memory = get_memory_usage()
        
        model_overhead = model_memory - baseline_memory
        print(f"æ¨¡å‹å†…å­˜å ç”¨: {model_overhead:.1f} MB")
        
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„å†…å­˜ä½¿ç”¨
        memory_results = {}
        
        for batch_size in [1, 4, 8, 16]:
            data = performance_data[batch_size]
            
            # å‰å‘ä¼ æ’­
            model.eval()
            with torch.no_grad():
                output = model(h_path=data['path_features'], h_omic=data['omic_features'])
            
            current_memory = get_memory_usage()
            batch_memory = current_memory - model_memory
            
            memory_results[batch_size] = {
                'total_memory': current_memory,
                'batch_memory': batch_memory,
                'memory_per_sample': batch_memory / batch_size
            }
            
            print(f"Batch Size {batch_size}: {batch_memory:.1f} MB ({batch_memory/batch_size:.1f} MB/sample)")
        
        # éªŒè¯å†…å­˜ä½¿ç”¨åˆç†æ€§
        for batch_size, metrics in memory_results.items():
            # æ¯æ ·æœ¬å†…å­˜ä½¿ç”¨åº”å°äº50MB
            assert metrics['memory_per_sample'] < 50, f"å†…å­˜ä½¿ç”¨è¿‡é«˜: {metrics['memory_per_sample']:.1f}MB/sample"
        
        print("å†…å­˜ä½¿ç”¨æµ‹è¯•é€šè¿‡")
    
    def test_scalability(self):
        """æµ‹è¯•å¯æ‰©å±•æ€§"""
        model = PorpoiseMMF(omic_input_dim=50, path_input_dim=768, n_classes=4)
        model.eval()
        
        # æµ‹è¯•ä¸åŒè¾“å…¥å¤§å°çš„å¤„ç†èƒ½åŠ›
        patch_counts = [10, 50, 100, 200, 500]
        scalability_results = {}
        
        for patch_count in patch_counts:
            path_features = torch.randn(1, patch_count, 768)
            omic_features = torch.randn(1, 50)
            
            # æµ‹è¯•å¤„ç†æ—¶é—´
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(10):  # é‡å¤10æ¬¡å–å¹³å‡
                    output = model(h_path=path_features, h_omic=omic_features)
            
            end_time = time.time()
            avg_time = (end_time - start_time) / 10
            
            scalability_results[patch_count] = {
                'avg_time': avg_time,
                'time_per_patch': avg_time / patch_count
            }
            
            print(f"Patch Count {patch_count}: {avg_time*1000:.1f}ms ({avg_time/patch_count*1000:.3f}ms/patch)")
        
        # éªŒè¯å¯æ‰©å±•æ€§
        times = [metrics['avg_time'] for metrics in scalability_results.values()]
        
        # å¤„ç†æ—¶é—´åº”è¯¥éšè¾“å…¥å¤§å°åˆç†å¢é•¿ï¼ˆä¸åº”è¯¥æ˜¯æŒ‡æ•°å¢é•¿ï¼‰
        time_ratios = [times[i+1]/times[i] for i in range(len(times)-1)]
        max_ratio = max(time_ratios)
        
        assert max_ratio < 10, f"å¯æ‰©å±•æ€§å·®ï¼Œæ—¶é—´å¢é•¿æ¯”ä¾‹è¿‡å¤§: {max_ratio:.1f}"
        print("å¯æ‰©å±•æ€§æµ‹è¯•é€šè¿‡")
        
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
EOF

python test_performance_benchmark.py
```

#### 5.2 å†…å­˜ä½¿ç”¨æµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/performance/test_memory_usage.py`

#### 5.3 å¯æ‰©å±•æ€§æµ‹è¯•
**æ–‡ä»¶ä½ç½®**ï¼š`tests/performance/test_scalability.py`

### ä»»åŠ¡6ï¼šå®ç°å›å½’æµ‹è¯•

#### 6.1 åˆ›å»ºå›å½’æµ‹è¯•å¥—ä»¶
**æ–‡ä»¶ä½ç½®**ï¼š`tests/regression/test_regression_suite.py`

**æµ‹è¯•å†…å®¹**ï¼š
- æ¨¡å‹è¾“å‡ºä¸€è‡´æ€§æµ‹è¯•
- æ€§èƒ½å›å½’æµ‹è¯•
- APIå…¼å®¹æ€§æµ‹è¯•
- é…ç½®å…¼å®¹æ€§æµ‹è¯•

#### 6.2 åˆ›å»ºåŸºå‡†ç»“æœ
**æ–‡ä»¶ä½ç½®**ï¼š`tests/regression/baseline_results.json`

**å†…å®¹**ï¼š
- å„æ¨¡å‹çš„åŸºå‡†æ€§èƒ½æŒ‡æ ‡
- æ ‡å‡†æµ‹è¯•æ•°æ®çš„é¢„æœŸè¾“å‡º
- æ€§èƒ½åŸºå‡†æ•°æ®

### ä»»åŠ¡7ï¼šå®ç°æŒç»­é›†æˆ

#### 7.1 åˆ›å»ºCIé…ç½®
**æ–‡ä»¶ä½ç½®**ï¼š`.github/workflows/ci.yml`

**å®ç°è¦æ±‚**ï¼š
- è‡ªåŠ¨åŒ–æµ‹è¯•æ‰§è¡Œ
- å¤šPythonç‰ˆæœ¬æµ‹è¯•
- ä»£ç è¦†ç›–ç‡æŠ¥å‘Š
- æ€§èƒ½å›å½’æ£€æµ‹

#### 7.2 åˆ›å»ºæµ‹è¯•è„šæœ¬
**æ–‡ä»¶ä½ç½®**ï¼š`scripts/run_tests.py`

**å®ç°è¦æ±‚**ï¼š
- å®Œæ•´æµ‹è¯•å¥—ä»¶æ‰§è¡Œ
- æµ‹è¯•ç»“æœæ±‡æ€»
- å¤±è´¥æµ‹è¯•æŠ¥å‘Š
- è¦†ç›–ç‡åˆ†æ

**æµ‹è¯•æ–¹æ³•**ï¼š
```bash
# åˆ›å»ºæµ‹è¯•æ‰§è¡Œè„šæœ¬
cat > run_comprehensive_tests.py << 'EOF'
import subprocess
import sys
import os
import time

def run_test_suite(test_type, test_path, description):
    """è¿è¡Œæµ‹è¯•å¥—ä»¶"""
    print(f"\n{'='*60}")
    print(f"è¿è¡Œ {description}")
    print(f"{'='*60}")
    
    start_time = time.time()
    
    try:
        result = subprocess.run(
            [sys.executable, '-m', 'pytest', test_path, '-v', '--tb=short'],
            capture_output=True,
            text=True,
            timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
        )
        
        end_time = time.time()
        duration = end_time - start_time
        
        print(f"æ‰§è¡Œæ—¶é—´: {duration:.1f}ç§’")
        
        if result.returncode == 0:
            print(f"âœ… {description} é€šè¿‡")
            return True
        else:
            print(f"âŒ {description} å¤±è´¥")
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print(f"â° {description} è¶…æ—¶")
        return False
    except Exception as e:
        print(f"ğŸ’¥ {description} æ‰§è¡Œé”™è¯¯: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("MMSurv é¡¹ç›®ç»¼åˆæµ‹è¯•å¥—ä»¶")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # æµ‹è¯•å¥—ä»¶å®šä¹‰
    test_suites = [
        ('unit', 'tests/unit/', 'å•å…ƒæµ‹è¯•'),
        ('integration', 'tests/integration/', 'é›†æˆæµ‹è¯•'),
        ('system', 'tests/system/', 'ç³»ç»Ÿæµ‹è¯•'),
        ('performance', 'tests/performance/', 'æ€§èƒ½æµ‹è¯•')
    ]
    
    results = {}
    total_start_time = time.time()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•å¥—ä»¶
    for test_type, test_path, description in test_suites:
        if os.path.exists(test_path):
            results[test_type] = run_test_suite(test_type, test_path, description)
        else:
            print(f"âš ï¸  è·³è¿‡ {description} - ç›®å½•ä¸å­˜åœ¨: {test_path}")
            results[test_type] = None
    
    total_end_time = time.time()
    total_duration = total_end_time - total_start_time
    
    # æ±‡æ€»ç»“æœ
    print(f"\n{'='*60}")
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print(f"{'='*60}")
    
    passed = 0
    failed = 0
    skipped = 0
    
    for test_type, result in results.items():
        if result is True:
            print(f"âœ… {test_type.upper()}: é€šè¿‡")
            passed += 1
        elif result is False:
            print(f"âŒ {test_type.upper()}: å¤±è´¥")
            failed += 1
        else:
            print(f"âš ï¸  {test_type.upper()}: è·³è¿‡")
            skipped += 1
    
    print(f"\næ€»æ‰§è¡Œæ—¶é—´: {total_duration:.1f}ç§’")
    print(f"é€šè¿‡: {passed}, å¤±è´¥: {failed}, è·³è¿‡: {skipped}")
    
    # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
    try:
        print("\nç”Ÿæˆä»£ç è¦†ç›–ç‡æŠ¥å‘Š...")
        subprocess.run([sys.executable, '-m', 'coverage', 'report'], check=True)
    except (subprocess.CalledProcessError, FileNotFoundError):
        print("âš ï¸  æ— æ³•ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Šï¼ˆcoverageæœªå®‰è£…æˆ–æœªé…ç½®ï¼‰")
    
    # è¿”å›é€€å‡ºç 
    if failed > 0:
        print("\nâŒ æµ‹è¯•å¥—ä»¶æ‰§è¡Œå¤±è´¥")
        sys.exit(1)
    else:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡")
        sys.exit(0)

if __name__ == "__main__":
    main()
EOF

python run_comprehensive_tests.py
```

## æµ‹è¯•æ‰§è¡Œå’ŒæŠ¥å‘Š

### è¿è¡Œæ‰€æœ‰æµ‹è¯•

#### åŸºç¡€æµ‹è¯•æ‰§è¡Œ
```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m pytest tests/ -v

# è¿è¡Œç‰¹å®šç±»å‹çš„æµ‹è¯•
python -m pytest tests/unit/ -v
python -m pytest tests/integration/ -v
python -m pytest tests/system/ -v
python -m pytest tests/performance/ -v

# è¿è¡Œç‰¹å®šæ¨¡å—æµ‹è¯•
python -m pytest tests/unit/test_models/ -v
python -m pytest tests/unit/test_datasets/ -v
```

#### è¦†ç›–ç‡æµ‹è¯•
```bash
# å®‰è£…è¦†ç›–ç‡å·¥å…·
pip install coverage pytest-cov

# è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
python -m pytest tests/ --cov=mmsurv --cov-report=html --cov-report=term

# æŸ¥çœ‹è¦†ç›–ç‡æŠ¥å‘Š
open htmlcov/index.html  # åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹
```

#### æ€§èƒ½åŸºå‡†æµ‹è¯•
```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
python -m pytest tests/performance/ -v --benchmark-only

# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
python -m pytest tests/performance/ --benchmark-json=benchmark_results.json
```

### æµ‹è¯•æŠ¥å‘Šç”Ÿæˆ

#### HTMLæµ‹è¯•æŠ¥å‘Š
```bash
# å®‰è£…æŠ¥å‘Šç”Ÿæˆå·¥å…·
pip install pytest-html

# ç”ŸæˆHTMLæŠ¥å‘Š
python -m pytest tests/ --html=test_report.html --self-contained-html
```

#### JUnit XMLæŠ¥å‘Š
```bash
# ç”ŸæˆJUnit XMLæŠ¥å‘Šï¼ˆç”¨äºCI/CDï¼‰
python -m pytest tests/ --junitxml=test_results.xml
```

## è´¨é‡ä¿è¯

### ä»£ç è´¨é‡æ£€æŸ¥

#### é™æ€ä»£ç åˆ†æ
```bash
# å®‰è£…ä»£ç è´¨é‡å·¥å…·
pip install flake8 black isort mypy

# ä»£ç æ ¼å¼æ£€æŸ¥
flake8 mmsurv/ tests/

# ä»£ç æ ¼å¼åŒ–
black mmsurv/ tests/

# å¯¼å…¥æ’åº
isort mmsurv/ tests/

# ç±»å‹æ£€æŸ¥
mypy mmsurv/
```

#### å®‰å…¨æ£€æŸ¥
```bash
# å®‰è£…å®‰å…¨æ£€æŸ¥å·¥å…·
pip install bandit safety

# å®‰å…¨æ¼æ´æ£€æŸ¥
bandit -r mmsurv/

# ä¾èµ–å®‰å…¨æ£€æŸ¥
safety check
```

### æ€§èƒ½ç›‘æ§

#### å†…å­˜æ³„æ¼æ£€æµ‹
```bash
# å®‰è£…å†…å­˜åˆ†æå·¥å…·
pip install memory-profiler

# å†…å­˜ä½¿ç”¨åˆ†æ
python -m memory_profiler scripts/train.py
```

#### æ€§èƒ½åˆ†æ
```bash
# å®‰è£…æ€§èƒ½åˆ†æå·¥å…·
pip install py-spy

# æ€§èƒ½åˆ†æ
py-spy record -o profile.svg -- python scripts/train.py
```

## æ•…éšœæ’é™¤

### å¸¸è§æµ‹è¯•é—®é¢˜

#### é—®é¢˜1ï¼šæµ‹è¯•ç¯å¢ƒé…ç½®é”™è¯¯
**ç—‡çŠ¶**ï¼šImportErroræˆ–ModuleNotFoundError
**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥PYTHONPATHè®¾ç½®
2. ç¡®è®¤è™šæ‹Ÿç¯å¢ƒæ¿€æ´»
3. éªŒè¯ä¾èµ–åŒ…å®‰è£…
4. æ£€æŸ¥æµ‹è¯•æ–‡ä»¶è·¯å¾„

#### é—®é¢˜2ï¼šæµ‹è¯•æ•°æ®é—®é¢˜
**ç—‡çŠ¶**ï¼šæµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥æˆ–æ ¼å¼é”™è¯¯
**è§£å†³æ–¹æ¡ˆ**ï¼š
1. æ£€æŸ¥æµ‹è¯•æ•°æ®è·¯å¾„
2. éªŒè¯æ•°æ®æ ¼å¼
3. ç¡®è®¤æ–‡ä»¶æƒé™
4. ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æ›¿ä»£

#### é—®é¢˜3ï¼šæµ‹è¯•è¶…æ—¶
**ç—‡çŠ¶**ï¼šæµ‹è¯•æ‰§è¡Œæ—¶é—´è¿‡é•¿
**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å°‘æµ‹è¯•æ•°æ®å¤§å°
2. ä¼˜åŒ–æµ‹è¯•é€»è¾‘
3. å¢åŠ è¶…æ—¶æ—¶é—´
4. å¹¶è¡Œæ‰§è¡Œæµ‹è¯•

#### é—®é¢˜4ï¼šéšæœºæ€§æµ‹è¯•å¤±è´¥
**ç—‡çŠ¶**ï¼šæµ‹è¯•ç»“æœä¸ç¨³å®š
**è§£å†³æ–¹æ¡ˆ**ï¼š
1. è®¾ç½®å›ºå®šéšæœºç§å­
2. ä½¿ç”¨ç»Ÿè®¡æµ‹è¯•æ–¹æ³•
3. å¢åŠ æµ‹è¯•æ ·æœ¬æ•°é‡
4. è®¾ç½®åˆç†çš„å®¹å·®èŒƒå›´

## éªŒè¯æ¸…å•

å®Œæˆæœ¬é˜¶æ®µåï¼Œè¯·ç¡®è®¤ä»¥ä¸‹é¡¹ç›®ï¼š

- [ ] æµ‹è¯•æ¡†æ¶å»ºç«‹å®Œæˆ
- [ ] å•å…ƒæµ‹è¯•å®ç°å®Œæˆ
- [ ] é›†æˆæµ‹è¯•å®ç°å®Œæˆ
- [ ] ç³»ç»Ÿæµ‹è¯•å®ç°å®Œæˆ
- [ ] æ€§èƒ½æµ‹è¯•å®ç°å®Œæˆ
- [ ] å›å½’æµ‹è¯•å®ç°å®Œæˆ
- [ ] æŒç»­é›†æˆé…ç½®å®Œæˆ
- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡
- [ ] ä»£ç è¦†ç›–ç‡è¾¾åˆ°80%ä»¥ä¸Š
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡
- [ ] å®‰å…¨æ£€æŸ¥é€šè¿‡
- [ ] æ–‡æ¡£æµ‹è¯•é€šè¿‡
- [ ] æµ‹è¯•æŠ¥å‘Šç”ŸæˆæˆåŠŸ

## ä¸‹ä¸€æ­¥

å®Œæˆæµ‹è¯•å’ŒéªŒè¯åï¼Œæ‚¨çš„MMSurvé¡¹ç›®å°†å…·å¤‡ï¼š

1. **å®Œæ•´çš„æµ‹è¯•ä½“ç³»** - ä»å•å…ƒæµ‹è¯•åˆ°ç³»ç»Ÿæµ‹è¯•çš„å…¨è¦†ç›–
2. **è‡ªåŠ¨åŒ–è´¨é‡ä¿è¯** - æŒç»­é›†æˆå’Œè‡ªåŠ¨åŒ–æµ‹è¯•
3. **æ€§èƒ½ç›‘æ§** - æ€§èƒ½åŸºå‡†å’Œå›å½’æ£€æµ‹
4. **å¯é æ€§ä¿è¯** - å…¨é¢çš„é”™è¯¯æ£€æµ‹å’Œå¤„ç†
5. **å¯ç»´æŠ¤æ€§** - æ¸…æ™°çš„æµ‹è¯•ç»“æ„å’Œæ–‡æ¡£

è‡³æ­¤ï¼Œæ‚¨å·²ç»å®Œæˆäº†MMSurvå¤šæ¨¡æ€ç”Ÿå­˜é¢„æµ‹é¡¹ç›®çš„å®Œæ•´å¤ç°ï¼

## æ€»ç»“

é€šè¿‡æœ¬æ–‡æ¡£çš„æŒ‡å¯¼ï¼Œæ‚¨å·²ç»å»ºç«‹äº†ä¸€ä¸ªå®Œæ•´çš„æµ‹è¯•å’ŒéªŒè¯ä½“ç³»ï¼Œç¡®ä¿é¡¹ç›®çš„è´¨é‡å’Œå¯é æ€§ã€‚è¿™ä¸ªæµ‹è¯•ä½“ç³»å°†å¸®åŠ©æ‚¨ï¼š

- åŠæ—©å‘ç°å’Œä¿®å¤é—®é¢˜
- ç¡®ä¿ä»£ç è´¨é‡å’Œæ€§èƒ½
- ç»´æŠ¤é¡¹ç›®çš„é•¿æœŸç¨³å®šæ€§
- æ”¯æŒæŒç»­å¼€å‘å’Œæ”¹è¿›

æ­å–œæ‚¨å®Œæˆäº†MMSurvé¡¹ç›®çš„å®Œæ•´å¤ç°ï¼